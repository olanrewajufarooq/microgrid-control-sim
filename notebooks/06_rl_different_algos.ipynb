{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7b3bd8",
   "metadata": {},
   "source": [
    "# 06 ‚Äî RL with Different Algorithms\n",
    "\n",
    "This notebook trains and evaluates multiple RL algorithms (PPO, A2C, SAC, TD3) on the same microgrid environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6288a3",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a44e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "# Path for local package\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Microgrid sim imports\n",
    "from microgrid_sim.core import MicrogridGymEnv, MicrogridEnv\n",
    "from microgrid_sim.data import SyntheticDataBuilder\n",
    "from microgrid_sim.utils import plot_simulation, plot_reward_progress\n",
    "from microgrid_sim.components import PVGenerator, WindTurbine, FossilGenerator, GridIntertie, BatteryStorage, ResidentialLoad, FactoryLoad\n",
    "from microgrid_sim.control import RLController\n",
    "\n",
    "# SB3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c65c2",
   "metadata": {},
   "source": [
    "## Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62edfa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n",
      "Control steps/episode: 120\n",
      "Total training timesteps: 12000\n"
     ]
    }
   ],
   "source": [
    "# --- Global constants ---\n",
    "NO_OF_EPISODES = 100\n",
    "TOTAL_HOURS    = 24 * 5   # 120 hours of synthetic data per dataset\n",
    "CONTROL_DT     = 60       # minutes per control decision\n",
    "SIM_DT         = 1        # simulation step minutes\n",
    "SEED           = 42\n",
    "\n",
    "CONTROL_STEPS_PER_EP = (TOTAL_HOURS * 60) // CONTROL_DT  # 120 control steps/episode\n",
    "TRAINING_TIMESTEPS   = CONTROL_STEPS_PER_EP * NO_OF_EPISODES\n",
    "\n",
    "# paths\n",
    "MODEL_DIR   = \"./models\"\n",
    "LOG_DIR     = \"./logs/algos\"\n",
    "RESULTS_DIR = \"./plots/06_rl_algos\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Config OK\")\n",
    "print(f\"Control steps/episode: {CONTROL_STEPS_PER_EP}\")\n",
    "print(f\"Total training timesteps: {TRAINING_TIMESTEPS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda37ae",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294d424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_builder(seed: int) -> SyntheticDataBuilder:\n",
    "    builder = SyntheticDataBuilder(\n",
    "        total_hours=TOTAL_HOURS,\n",
    "        sim_dt_minutes=SIM_DT,\n",
    "        seed=seed\n",
    "    )\n",
    "    # Names must match env components below\n",
    "    builder.add_pv(\"pv_1\", peak_irr=900)\n",
    "    builder.add_pv(\"pv_2\", peak_irr=500)\n",
    "    builder.add_wind(\"wind_a\", mean_speed=7.0)\n",
    "    builder.add_load(\"factory_a\", base_kw=20.0, profile=\"factory\")\n",
    "    builder.add_load(\"factory_b\", base_kw=13.5,  profile=\"factory\")\n",
    "    builder.add_load(\"house_a\",   base_kw=3.5,  profile=\"residential\")\n",
    "    builder.add_load(\"house_b\",   base_kw=2.5,  profile=\"residential\")\n",
    "    builder.add_load(\"house_c\",   base_kw=4.5,  profile=\"residential\")\n",
    "    builder.add_grid_prices(\"grid\")\n",
    "    return builder\n",
    "\n",
    "\n",
    "def setup_microgrid_env() -> MicrogridEnv:\n",
    "    env = MicrogridEnv(\n",
    "        simulation_hours=TOTAL_HOURS,\n",
    "        control_interval_minutes=CONTROL_DT,\n",
    "        sim_dt_minutes=SIM_DT\n",
    "    )\n",
    "\n",
    "    # Generators\n",
    "    pv1 = PVGenerator(\"pv_1\", capacity_kw=5.0,  time_step_minutes=SIM_DT)\n",
    "    pv2 = PVGenerator(\"pv_2\", capacity_kw=10.0,  time_step_minutes=SIM_DT)\n",
    "    wind_a = WindTurbine(\"wind_a\", rated_kw=10.5, time_step_minutes=SIM_DT)\n",
    "\n",
    "    # Dispatchable\n",
    "    diesel1 = FossilGenerator(\"diesel_1\", p_min_kw=2.0, p_max_kw=10.0,\n",
    "                              time_step_minutes=SIM_DT, fuel_cost_per_kwh=0.35)\n",
    "    diesel2 = FossilGenerator(\"diesel_2\", p_min_kw=0.0, p_max_kw=6.0,\n",
    "                              time_step_minutes=SIM_DT, fuel_cost_per_kwh=0.35)\n",
    "\n",
    "    # Loads\n",
    "    house_a = ResidentialLoad(\"house_a\", base_kw=3.5, noise_std=0.0)\n",
    "    house_b = ResidentialLoad(\"house_b\", base_kw=2.5, noise_std=0.0)\n",
    "    house_c = ResidentialLoad(\"house_c\", base_kw=4.5, noise_std=0.0)\n",
    "    factory_a = FactoryLoad(\"factory_a\", base_kw=20.0, noise_std=0.0)\n",
    "    factory_b = FactoryLoad(\"factory_b\", base_kw=13.5,  noise_std=0.0)\n",
    "    # Storage\n",
    "    bat1 = BatteryStorage(\"bat_1\", capacity_kwh=20.0, time_step_minutes=SIM_DT,\n",
    "                          max_charge_kw=8.0, max_discharge_kw=8.0)\n",
    "    bat2 = BatteryStorage(\"bat_2\", capacity_kwh=15.0, time_step_minutes=SIM_DT,\n",
    "                          max_charge_kw=5.0, max_discharge_kw=5.0)\n",
    "\n",
    "    # Grid\n",
    "    grid = GridIntertie(\"grid\", time_step_minutes=SIM_DT,\n",
    "                        import_limit_kw=25.0, export_limit_kw=10.0,\n",
    "                        price_export_per_kwh=0.05, price_import_per_kwh=0.15)\n",
    "\n",
    "    # Add\n",
    "    for c in [pv1, pv2, wind_a, diesel1, diesel2, house_a, house_b, house_c,\n",
    "              factory_a, factory_b, bat1, bat2]:\n",
    "        env.add_component(c)\n",
    "    env.add_component(grid, is_grid=True)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def build_wrapped_env(seed: int, log_csv_path: str):\n",
    "    \"\"\"\n",
    "    Returns: (train_eval_env_flat_and_monitored, orig_dict_env_reference)\n",
    "    - Flattened Box action env for SB3\n",
    "    - Monitor wrapper logs reward to CSV for our plots\n",
    "    - Keep a reference to the original dict env (to get DataFrame results)\n",
    "    \"\"\"\n",
    "    base_env = setup_microgrid_env()\n",
    "    data_builder = setup_data_builder(seed)\n",
    "\n",
    "    gym_env_dict = MicrogridGymEnv(\n",
    "        microgrid_env=base_env,\n",
    "        data_builder=data_builder,\n",
    "        reward_weights={\n",
    "            \"cost\": 5.0,\n",
    "            \"unmet\": 5.0,\n",
    "            \"curtailment\": 0.1,\n",
    "            \"soc_deviation\": 0.0\n",
    "        }\n",
    "    )\n",
    "    gym_env_flat = gym_env_dict.create_flattened_env()\n",
    "    env_mon = Monitor(gym_env_flat, filename=log_csv_path)\n",
    "\n",
    "    return env_mon, gym_env_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e9df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_episode(env_flat, policy, seed=SEED):\n",
    "    \"\"\"\n",
    "    Runs one full episode with a trained policy on a flattened env.\n",
    "    Returns (df_results, total_reward, steps).\n",
    "    \"\"\"\n",
    "    obs, info = env_flat.reset(seed=seed)\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action, _ = policy.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env_flat.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    df = env_flat.unwrapped.env.get_results(as_dataframe=True)\n",
    "    return df, float(total_reward), steps\n",
    "\n",
    "\n",
    "def summarize_results(df, sim_dt_minutes=SIM_DT):\n",
    "    dt_hours = sim_dt_minutes / 60.0\n",
    "    total_cost = float(df[\"total_cashflow\"].sum() if \"total_cashflow\" in df.columns else 0.0)\n",
    "    unmet_kwh = float(df[\"unmet_load_kw\"].sum() * dt_hours if \"unmet_load_kw\" in df.columns else 0.0)\n",
    "    curtailed_kwh = float(df[\"curtailed_gen_kw\"].sum() * dt_hours if \"curtailed_gen_kw\" in df.columns else 0.0)\n",
    "    return {\"Total Cost ($)\": total_cost, \"Unmet Energy (kWh)\": unmet_kwh, \"Curtailed Energy (kWh)\": curtailed_kwh}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c94ec",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d36cb",
   "metadata": {},
   "source": [
    "## Build PPO envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273d9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO envs ready\n"
     ]
    }
   ],
   "source": [
    "ppo_log_csv = os.path.join(LOG_DIR, \"PPO_monitor.csv\")\n",
    "ppo_env_train, ppo_env_dict_ref = build_wrapped_env(seed=SEED, log_csv_path=ppo_log_csv)\n",
    "\n",
    "# Use a separate test instance (same seed & config) to avoid train-state leakage\n",
    "ppo_env_test,  _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"PPO_test_monitor.csv\"))\n",
    "\n",
    "print(\"PPO envs ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514be008",
   "metadata": {},
   "source": [
    "## Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbf312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8295bccab584fd38bce6d2648138c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_controller = RLController(\n",
    "    algo=\"PPO\",\n",
    "    env=ppo_env_train,\n",
    "    policy='MultiInputPolicy',\n",
    "    n_steps=CONTROL_STEPS_PER_EP,\n",
    "    batch_size=12,\n",
    "    n_epochs=5,\n",
    ")\n",
    "\n",
    "ppo_controller.train(\n",
    "    total_timesteps=TRAINING_TIMESTEPS,\n",
    "    log_dir=LOG_DIR,\n",
    ")\n",
    "\n",
    "# ppo.learn(total_timesteps=TRAINING_TIMESTEPS, progress_bar=True)\n",
    "ppo_model_path = os.path.join(MODEL_DIR, \"ppo_microgrid\")\n",
    "ppo_controller.save(ppo_model_path)\n",
    "print(\"PPO trained & saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408300",
   "metadata": {},
   "source": [
    "## PPO reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_progress(\n",
    "    monitor_csv_path=ppo_log_csv,\n",
    "    title=\"PPO ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"ppo_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f97da",
   "metadata": {},
   "source": [
    "## PPO evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load for safety (optional)\n",
    "ppo_loaded = PPO.load(ppo_model_path, env=ppo_env_train)\n",
    "\n",
    "df_ppo, ppo_reward, ppo_steps = run_full_episode(ppo_env_test, ppo_loaded)\n",
    "ppo_metrics = summarize_results(df_ppo)\n",
    "\n",
    "print(\"PPO steps:\", ppo_steps)\n",
    "print(\"PPO reward:\", f\"{ppo_reward:.2f}\")\n",
    "print(\"PPO metrics:\", ppo_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_ppo,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-PPO\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16bdc",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb525b37",
   "metadata": {},
   "source": [
    "## Build A2C envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_log_csv   = os.path.join(LOG_DIR, \"A2C_monitor.csv\")\n",
    "a2c_env_train, a2c_env_dict_ref = build_wrapped_env(seed=SEED, log_csv_path=a2c_log_csv)\n",
    "a2c_env_test,  _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"A2C_test_monitor.csv\"))\n",
    "\n",
    "print(\"A2C envs ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1dab7",
   "metadata": {},
   "source": [
    "## Train A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c = A2C(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=a2c_env_train,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=CONTROL_STEPS_PER_EP,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    verbose=0,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "a2c.learn(total_timesteps=TRAINING_TIMESTEPS, progress_bar=True)\n",
    "a2c_model_path = os.path.join(MODEL_DIR, \"a2c_microgrid\")\n",
    "a2c.save(a2c_model_path)\n",
    "print(\"A2C trained & saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9b4b0",
   "metadata": {},
   "source": [
    "## A2C reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_progress(\n",
    "    monitor_csv_path=a2c_log_csv,\n",
    "    title=\"A2C ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"a2c_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a847c",
   "metadata": {},
   "source": [
    "## A2C evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_loaded = A2C.load(a2c_model_path, env=a2c_env_train)\n",
    "\n",
    "df_a2c, a2c_reward, a2c_steps = run_full_episode(a2c_env_test, a2c_loaded)\n",
    "a2c_metrics = summarize_results(df_a2c)\n",
    "\n",
    "print(\"A2C steps:\", a2c_steps)\n",
    "print(\"A2C reward:\", f\"{a2c_reward:.2f}\")\n",
    "print(\"A2C metrics:\", a2c_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_a2c,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-A2C\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201b60f",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488391d5",
   "metadata": {},
   "source": [
    "## Build SAC envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c95dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_log_csv   = os.path.join(LOG_DIR, \"SAC_monitor.csv\")\n",
    "sac_env_train, sac_env_dict_ref = build_wrapped_env(seed=SEED, log_csv_path=sac_log_csv)\n",
    "sac_env_test,  _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"SAC_test_monitor.csv\"))\n",
    "\n",
    "print(\"SAC envs ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c79653",
   "metadata": {},
   "source": [
    "## Train SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC expects Box actions (we have flattened Box) and supports MultiInputPolicy for Dict obs\n",
    "sac = SAC(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=sac_env_train,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    learning_starts=CONTROL_STEPS_PER_EP,  # after one episode of experience\n",
    "    ent_coef=\"auto\",\n",
    "    verbose=0,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "sac.learn(total_timesteps=TRAINING_TIMESTEPS, progress_bar=True)\n",
    "sac_model_path = os.path.join(MODEL_DIR, \"sac_microgrid\")\n",
    "sac.save(sac_model_path)\n",
    "print(\"SAC trained & saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267272",
   "metadata": {},
   "source": [
    "## SAC reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_progress(\n",
    "    monitor_csv_path=sac_log_csv,\n",
    "    title=\"SAC ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"sac_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc515b",
   "metadata": {},
   "source": [
    "## SAC evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_loaded = SAC.load(sac_model_path, env=sac_env_train)\n",
    "\n",
    "df_sac, sac_reward, sac_steps = run_full_episode(sac_env_test, sac_loaded)\n",
    "sac_metrics = summarize_results(df_sac)\n",
    "\n",
    "print(\"SAC steps:\", sac_steps)\n",
    "print(\"SAC reward:\", f\"{sac_reward:.2f}\")\n",
    "print(\"SAC metrics:\", sac_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_sac,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-SAC\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bec8d4",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bf113",
   "metadata": {},
   "source": [
    "## Build TD3 envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_log_csv   = os.path.join(LOG_DIR, \"TD3_monitor.csv\")\n",
    "td3_env_train, td3_env_dict_ref = build_wrapped_env(seed=SEED, log_csv_path=td3_log_csv)\n",
    "td3_env_test,  _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"TD3_test_monitor.csv\"))\n",
    "\n",
    "print(\"TD3 envs ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee169fda",
   "metadata": {},
   "source": [
    "## Train TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7eb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = TD3(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=td3_env_train,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    learning_starts=CONTROL_STEPS_PER_EP,\n",
    "    policy_delay=2,\n",
    "    verbose=0,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "td3.learn(total_timesteps=TRAINING_TIMESTEPS, progress_bar=True)\n",
    "td3_model_path = os.path.join(MODEL_DIR, \"td3_microgrid\")\n",
    "td3.save(td3_model_path)\n",
    "print(\"TD3 trained & saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69aa29",
   "metadata": {},
   "source": [
    "## TD3 reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53decdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_progress(\n",
    "    monitor_csv_path=td3_log_csv,\n",
    "    title=\"TD3 ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"td3_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7900b",
   "metadata": {},
   "source": [
    "## TD3 evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_loaded = TD3.load(td3_model_path, env=td3_env_train)\n",
    "\n",
    "df_td3, td3_reward, td3_steps = run_full_episode(td3_env_test, td3_loaded)\n",
    "td3_metrics = summarize_results(df_td3)\n",
    "\n",
    "print(\"TD3 steps:\", td3_steps)\n",
    "print(\"TD3 reward:\", f\"{td3_reward:.2f}\")\n",
    "print(\"TD3 metrics:\", td3_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_td3,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-TD3\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a8bce",
   "metadata": {},
   "source": [
    "# Random Policy Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0803cc",
   "metadata": {},
   "source": [
    "## Random policy (for context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56432bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_env_test, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"RANDOM_test_monitor.csv\"))\n",
    "\n",
    "obs, info = rand_env_test.reset(seed=SEED)\n",
    "rand_reward = 0.0\n",
    "done = False\n",
    "rand_steps = 0\n",
    "while not done:\n",
    "    action = rand_env_test.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = rand_env_test.step(action)\n",
    "    rand_reward += reward\n",
    "    rand_steps += 1\n",
    "    done = terminated or truncated\n",
    "\n",
    "df_random = rand_env_test.unwrapped.env.get_results(as_dataframe=True)\n",
    "rand_metrics = summarize_results(df_random)\n",
    "\n",
    "print(\"Random steps:\", rand_steps)\n",
    "print(\"Random reward:\", f\"{rand_reward:.2f}\")\n",
    "print(\"Random metrics:\", rand_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_random,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-RANDOM\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e9843",
   "metadata": {},
   "source": [
    "# Comparison of All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea67e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_money(x):\n",
    "    return f\"${x:,.2f}\"\n",
    "\n",
    "results = {\n",
    "    \"Random\": {**rand_metrics, \"Test Reward\": rand_reward},\n",
    "    \"PPO\":    {**ppo_metrics,  \"Test Reward\": ppo_reward},\n",
    "    \"A2C\":    {**a2c_metrics,  \"Test Reward\": a2c_reward},\n",
    "    \"SAC\":    {**sac_metrics,  \"Test Reward\": sac_reward},\n",
    "    \"TD3\":    {**td3_metrics,  \"Test Reward\": td3_reward},\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df[\"Total Cost ($)\"] = comparison_df[\"Total Cost ($)\"].apply(fmt_money)\n",
    "comparison_df[\"Unmet Energy (kWh)\"] = comparison_df[\"Unmet Energy (kWh)\"].apply(lambda v: f\"{v:.3f}\")\n",
    "comparison_df[\"Curtailed Energy (kWh)\"] = comparison_df[\"Curtailed Energy (kWh)\"].apply(lambda v: f\"{v:.3f}\")\n",
    "comparison_df[\"Test Reward\"] = comparison_df[\"Test Reward\"].apply(lambda v: f\"{v:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ ALGORITHM COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microgrid-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
