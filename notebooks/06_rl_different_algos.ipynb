{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7b3bd8",
   "metadata": {},
   "source": [
    "# 06 ‚Äî RL with Different Algorithms\n",
    "\n",
    "This notebook trains and evaluates multiple RL algorithms (PPO, A2C, SAC, TD3) on the same microgrid environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6288a3",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a44e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "# Path for local package\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Microgrid sim imports\n",
    "from microgrid_sim.core import MicrogridGymEnv, MicrogridEnv\n",
    "from microgrid_sim.data import SyntheticDataBuilder\n",
    "from microgrid_sim.utils import plot_simulation, plot_reward_progress\n",
    "from microgrid_sim.components import PVGenerator, WindTurbine, FossilGenerator, GridIntertie, BatteryStorage, ResidentialLoad, FactoryLoad\n",
    "from microgrid_sim.control import RLController\n",
    "\n",
    "# SB3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c65c2",
   "metadata": {},
   "source": [
    "## Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edfa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n",
      "Control steps/episode: 120\n",
      "Total training timesteps: 240000\n"
     ]
    }
   ],
   "source": [
    "# --- Global constants ---\n",
    "NO_OF_EPISODES = 1000\n",
    "TOTAL_HOURS    = 24 * 5   # 120 hours of synthetic data per dataset\n",
    "CONTROL_DT     = 60       # minutes per control decision\n",
    "SIM_DT         = 1        # simulation step minutes\n",
    "SEED           = 42\n",
    "\n",
    "CONTROL_STEPS_PER_EP = (TOTAL_HOURS * 60) // CONTROL_DT  # 120 control steps/episode\n",
    "TRAINING_TIMESTEPS   = CONTROL_STEPS_PER_EP * NO_OF_EPISODES\n",
    "\n",
    "# Reward\n",
    "REWARD_WEIGHTS = {\n",
    "    \"cost\": 50.0,\n",
    "    \"unmet\": 50.0,\n",
    "    \"curtailment\": 1.0,\n",
    "    \"soc_deviation\": 0.0\n",
    "}\n",
    "\n",
    "# paths\n",
    "MODEL_DIR   = \"./models\"\n",
    "LOG_DIR     = \"./logs/algos\"\n",
    "RESULTS_DIR = \"./plots/06_rl_algos\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "USE_SAVED_MODELS = False\n",
    "global_data = {\n",
    "    \"results\": {},\n",
    "    \"envs\": {},\n",
    "    \"models\": {}\n",
    "}\n",
    "\n",
    "print(\"Config OK\")\n",
    "print(f\"Control steps/episode: {CONTROL_STEPS_PER_EP}\")\n",
    "print(f\"Total training timesteps: {TRAINING_TIMESTEPS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda37ae",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294d424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_builder(seed: int) -> SyntheticDataBuilder:\n",
    "    builder = SyntheticDataBuilder(\n",
    "        total_hours=TOTAL_HOURS,\n",
    "        sim_dt_minutes=SIM_DT,\n",
    "        seed=seed\n",
    "    )\n",
    "    # Names must match env components below\n",
    "    builder.add_pv(\"pv_1\", peak_irr=900)\n",
    "    builder.add_pv(\"pv_2\", peak_irr=500)\n",
    "    builder.add_wind(\"wind_a\", mean_speed=7.0)\n",
    "    builder.add_load(\"factory_a\", base_kw=20.0, profile=\"factory\")\n",
    "    builder.add_load(\"factory_b\", base_kw=13.5,  profile=\"factory\")\n",
    "    builder.add_load(\"house_a\",   base_kw=3.5,  profile=\"residential\")\n",
    "    builder.add_load(\"house_b\",   base_kw=2.5,  profile=\"residential\")\n",
    "    builder.add_load(\"house_c\",   base_kw=4.5,  profile=\"residential\")\n",
    "    builder.add_grid_prices(\"grid\")\n",
    "    return builder\n",
    "\n",
    "\n",
    "def setup_microgrid_env() -> MicrogridEnv:\n",
    "    env = MicrogridEnv(\n",
    "        simulation_hours=TOTAL_HOURS,\n",
    "        control_interval_minutes=CONTROL_DT,\n",
    "        sim_dt_minutes=SIM_DT\n",
    "    )\n",
    "\n",
    "    # Generators\n",
    "    pv1 = PVGenerator(\"pv_1\", capacity_kw=20.0,  time_step_minutes=SIM_DT)\n",
    "    pv2 = PVGenerator(\"pv_2\", capacity_kw=25.0,  time_step_minutes=SIM_DT)\n",
    "    wind_a = WindTurbine(\"wind_a\", rated_kw=17.5, time_step_minutes=SIM_DT)\n",
    "\n",
    "    # Dispatchable\n",
    "    diesel1 = FossilGenerator(\"diesel_1\", p_min_kw=0.0, p_max_kw=20.0,\n",
    "                              time_step_minutes=SIM_DT, fuel_cost_per_kwh=0.35)\n",
    "    diesel2 = FossilGenerator(\"diesel_2\", p_min_kw=0.0, p_max_kw=15.0,\n",
    "                              time_step_minutes=SIM_DT, fuel_cost_per_kwh=0.35)\n",
    "\n",
    "    # Loads\n",
    "    house_a = ResidentialLoad(\"house_a\", base_kw=3.5, noise_std=0.0)\n",
    "    house_b = ResidentialLoad(\"house_b\", base_kw=2.5, noise_std=0.0)\n",
    "    house_c = ResidentialLoad(\"house_c\", base_kw=4.5, noise_std=0.0)\n",
    "    factory_a = FactoryLoad(\"factory_a\", base_kw=20.0, noise_std=0.0)\n",
    "    factory_b = FactoryLoad(\"factory_b\", base_kw=13.5,  noise_std=0.0)\n",
    "    # Storage\n",
    "    bat1 = BatteryStorage(\"bat_1\", capacity_kwh=50.0, time_step_minutes=SIM_DT,\n",
    "                          max_charge_kw=40.0, max_discharge_kw=35.0)\n",
    "    bat2 = BatteryStorage(\"bat_2\", capacity_kwh=30.0, time_step_minutes=SIM_DT,\n",
    "                          max_charge_kw=20.0, max_discharge_kw=15.0)\n",
    "\n",
    "    # Grid\n",
    "    grid = GridIntertie(\"grid\", time_step_minutes=SIM_DT,\n",
    "                        import_limit_kw=50.0, export_limit_kw=30.0,\n",
    "                        price_export_per_kwh=0.15, price_import_per_kwh=0.25)\n",
    "\n",
    "    # Add\n",
    "    for c in [pv1, pv2, wind_a, diesel1, diesel2, house_a, house_b, house_c,\n",
    "              factory_a, factory_b, bat1, bat2]:\n",
    "        env.add_component(c)\n",
    "    env.add_component(grid, is_grid=True)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def build_wrapped_env(seed: int, log_csv_path: str):\n",
    "    \"\"\"\n",
    "    Returns: (train_eval_env_flat_and_monitored, orig_dict_env_reference)\n",
    "    - Flattened Box action env for SB3\n",
    "    - Monitor wrapper logs reward to CSV for our plots\n",
    "    - Keep a reference to the original dict env (to get DataFrame results)\n",
    "    \"\"\"\n",
    "    base_env = setup_microgrid_env()\n",
    "    data_builder = setup_data_builder(seed)\n",
    "\n",
    "    gym_env_dict = MicrogridGymEnv(\n",
    "        microgrid_env=base_env,\n",
    "        data_builder=data_builder,\n",
    "        reward_weights=REWARD_WEIGHTS,\n",
    "    )\n",
    "    gym_env_flat = gym_env_dict.create_flattened_env()\n",
    "    env_mon = Monitor(gym_env_flat, filename=log_csv_path)\n",
    "\n",
    "    return env_mon, gym_env_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e9df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_episode(env_flat, policy, seed=SEED):\n",
    "    \"\"\"\n",
    "    Runs one full episode with a trained policy on a flattened env.\n",
    "    Returns (df_results, total_reward, steps).\n",
    "    \"\"\"\n",
    "    obs, info = env_flat.reset(seed=seed)\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action, _ = policy.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env_flat.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    df = env_flat.unwrapped.env.get_results(as_dataframe=True)\n",
    "    return df, float(total_reward), steps\n",
    "\n",
    "\n",
    "def summarize_results(df, sim_dt_minutes=SIM_DT):\n",
    "    dt_hours = sim_dt_minutes / 60.0\n",
    "    total_cost = float(df[\"total_cashflow\"].sum() if \"total_cashflow\" in df.columns else 0.0)\n",
    "    unmet_kwh = float(df[\"unmet_load_kw\"].sum() * dt_hours if \"unmet_load_kw\" in df.columns else 0.0)\n",
    "    curtailed_kwh = float(df[\"curtailed_gen_kw\"].sum() * dt_hours if \"curtailed_gen_kw\" in df.columns else 0.0)\n",
    "    return {\"Total Cost ($)\": total_cost, \"Unmet Energy (kWh)\": unmet_kwh, \"Curtailed Energy (kWh)\": curtailed_kwh}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec42774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_training(AlgoClass, algo_name, env_train, total_timesteps, **kwargs):\n",
    "    \"\"\"Initializes and trains/loads the controller, returning the policy and model path.\"\"\"\n",
    "\n",
    "    model_path = os.path.join(MODEL_DIR, f\"{algo_name}_microgrid\")\n",
    "    model_file = f\"{model_path}.zip\"\n",
    "    policy = None\n",
    "\n",
    "    # 1. Attempt to Load\n",
    "    if USE_SAVED_MODELS and os.path.exists(model_file):\n",
    "        try:\n",
    "            print(f\"\\n--- {algo_name}: Loading existing model ---\")\n",
    "            policy = AlgoClass.load(model_path, env=env_train, algo=algo_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {algo_name}: {e}. Training new model.\")\n",
    "            policy = None\n",
    "\n",
    "    # 2. Train if Loading Failed or Not Allowed\n",
    "    if policy is None:\n",
    "        print(f\"\\n--- {algo_name}: Building and training new model ---\")\n",
    "\n",
    "        rl_controller = RLController(\n",
    "            algo=algo_name, env=env_train, **kwargs\n",
    "        )\n",
    "\n",
    "        rl_controller.train(total_timesteps=total_timesteps, log_dir=LOG_DIR)\n",
    "        print(f\"{algo_name} training complete. Saving model...\")\n",
    "\n",
    "        rl_controller.save(model_path)\n",
    "        policy = rl_controller.model\n",
    "        print(f\"{algo_name} trained and saved.\")\n",
    "\n",
    "    return policy, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479a380",
   "metadata": {},
   "source": [
    "# Setup all environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ef4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments for training and testing set up.\n"
     ]
    }
   ],
   "source": [
    "# Global setup of environments (run once)\n",
    "ppo_env_train_vec, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"PPO_monitor.csv\"))\n",
    "a2c_env_train_vec, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"A2C_monitor.csv\"))\n",
    "sac_env_train_vec, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"SAC_monitor.csv\"))\n",
    "td3_env_train_vec, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"TD3_monitor.csv\"))\n",
    "\n",
    "ppo_env_test_vec, _ = build_wrapped_env(seed=SEED + 1, log_csv_path=os.path.join(LOG_DIR, \"PPO_test_monitor.csv\"))\n",
    "a2c_env_test_vec, _ = build_wrapped_env(seed=SEED + 1, log_csv_path=os.path.join(LOG_DIR, \"A2C_test_monitor.csv\"))\n",
    "sac_env_test_vec, _ = build_wrapped_env(seed=SEED + 1, log_csv_path=os.path.join(LOG_DIR, \"SAC_test_monitor.csv\"))\n",
    "td3_env_test_vec, _ = build_wrapped_env(seed=SEED + 1, log_csv_path=os.path.join(LOG_DIR, \"TD3_test_monitor.csv\"))\n",
    "\n",
    "print(\"Environments for training and testing set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c94ec",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514be008",
   "metadata": {},
   "source": [
    "## Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dbf312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PPO: Building and training new model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b325aeb3fc904c05ad510b244bf12a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_policy, ppo_model_path = manage_training(PPO, \"PPO\", ppo_env_train_vec, TRAINING_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408300",
   "metadata": {},
   "source": [
    "## PPO reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_log_csv = os.path.join(LOG_DIR, \"PPO_monitor.csv\")\n",
    "_ = plot_reward_progress(\n",
    "    monitor_csv_path=ppo_log_csv,\n",
    "    title=\"PPO ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"ppo_reward_progress.png\"),\n",
    "    rolling=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f97da",
   "metadata": {},
   "source": [
    "## PPO evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppo, ppo_reward, ppo_steps = run_full_episode(ppo_env_test_vec, ppo_policy, seed=SEED+1)\n",
    "ppo_metrics = summarize_results(df_ppo)\n",
    "global_data[\"results\"][\"PPO\"] = {**ppo_metrics, \"Test Reward\": ppo_reward}\n",
    "\n",
    "print(\"PPO reward:\", f\"{ppo_reward:.2f}\")\n",
    "print(\"PPO metrics:\", ppo_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_ppo, sim_dt_minutes=SIM_DT, sim_name=\"06-PPO\", save=True, base_dir=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da16bdc",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1dab7",
   "metadata": {},
   "source": [
    "## Train A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_policy, a2c_model_path = manage_training(A2C, \"A2C\", a2c_env_train_vec, TRAINING_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9b4b0",
   "metadata": {},
   "source": [
    "## A2C reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_log_csv = os.path.join(LOG_DIR, \"A2C_monitor.csv\")\n",
    "_ = plot_reward_progress(\n",
    "    monitor_csv_path=a2c_log_csv,\n",
    "    title=\"A2C ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"a2c_reward_progress.png\"),\n",
    "    rolling=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a847c",
   "metadata": {},
   "source": [
    "## A2C evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2c, a2c_reward, a2c_steps = run_full_episode(a2c_env_test_vec, a2c_policy, seed=SEED+1)\n",
    "a2c_metrics = summarize_results(df_a2c)\n",
    "global_data[\"results\"][\"A2C\"] = {**a2c_metrics, \"Test Reward\": a2c_reward}\n",
    "\n",
    "print(\"A2C reward:\", f\"{a2c_reward:.2f}\")\n",
    "print(\"A2C metrics:\", a2c_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_a2c, sim_dt_minutes=SIM_DT, sim_name=\"06-A2C\", save=True, base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201b60f",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c79653",
   "metadata": {},
   "source": [
    "## Train SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_policy, sac_model_path = manage_training(SAC, \"SAC\", sac_env_train_vec, TRAINING_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267272",
   "metadata": {},
   "source": [
    "## SAC reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_log_csv = os.path.join(LOG_DIR, \"SAC_monitor.csv\")\n",
    "_ = plot_reward_progress(\n",
    "    monitor_csv_path=sac_log_csv,\n",
    "    title=\"SAC ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"sac_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc515b",
   "metadata": {},
   "source": [
    "## SAC evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac, sac_reward, sac_steps = run_full_episode(sac_env_test_vec, sac_policy, seed=SEED+1)\n",
    "sac_metrics = summarize_results(df_sac)\n",
    "global_data[\"results\"][\"SAC\"] = {**sac_metrics, \"Test Reward\": sac_reward}\n",
    "\n",
    "print(\"SAC reward:\", f\"{sac_reward:.2f}\")\n",
    "print(\"SAC metrics:\", sac_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_sac, sim_dt_minutes=SIM_DT, sim_name=\"06-SAC\", save=True, base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bec8d4",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee169fda",
   "metadata": {},
   "source": [
    "## Train TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7eb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_policy, td3_model_path = manage_training(TD3, \"TD3\", td3_env_train_vec, TRAINING_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69aa29",
   "metadata": {},
   "source": [
    "## TD3 reward progression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53decdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_log_csv = os.path.join(LOG_DIR, \"TD3_monitor.csv\")\n",
    "_ = plot_reward_progress(\n",
    "    monitor_csv_path=td3_log_csv,\n",
    "    title=\"TD3 ‚Äî Training Reward\",\n",
    "    out_path=os.path.join(RESULTS_DIR, \"td3_reward_progress.png\"),\n",
    "    rolling=10\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7900b",
   "metadata": {},
   "source": [
    "## TD3 evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td3, td3_reward, td3_steps = run_full_episode(td3_env_test_vec, td3_policy, seed=SEED+1)\n",
    "td3_metrics = summarize_results(df_td3)\n",
    "global_data[\"results\"][\"TD3\"] = {**td3_metrics, \"Test Reward\": td3_reward}\n",
    "\n",
    "print(\"TD3 reward:\", f\"{td3_reward:.2f}\")\n",
    "print(\"TD3 metrics:\", td3_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_td3, sim_dt_minutes=SIM_DT, sim_name=\"06-TD3\", save=True, base_dir=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a8bce",
   "metadata": {},
   "source": [
    "# Random Policy Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0803cc",
   "metadata": {},
   "source": [
    "## Random policy (for context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56432bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_env_test, _ = build_wrapped_env(seed=SEED, log_csv_path=os.path.join(LOG_DIR, \"RANDOM_test_monitor.csv\"))\n",
    "\n",
    "obs, info = rand_env_test.reset(seed=SEED)\n",
    "rand_reward = 0.0\n",
    "done = False\n",
    "rand_steps = 0\n",
    "while not done:\n",
    "    action = rand_env_test.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = rand_env_test.step(action)\n",
    "    rand_reward += reward\n",
    "    rand_steps += 1\n",
    "    done = terminated or truncated\n",
    "\n",
    "df_random = rand_env_test.unwrapped.env.get_results(as_dataframe=True)\n",
    "rand_metrics = summarize_results(df_random)\n",
    "\n",
    "global_data[\"results\"][\"Random\"] = {**rand_metrics, \"Test Reward\": rand_reward}\n",
    "\n",
    "print(\"Random steps:\", rand_steps)\n",
    "print(\"Random reward:\", f\"{rand_reward:.2f}\")\n",
    "print(\"Random metrics:\", rand_metrics)\n",
    "\n",
    "_ = plot_simulation(\n",
    "    df_random,\n",
    "    sim_dt_minutes=SIM_DT,\n",
    "    sim_name=\"06-RANDOM\",\n",
    "    save=True,\n",
    "    base_dir=RESULTS_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e9843",
   "metadata": {},
   "source": [
    "# Comparison of All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea67e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_money(x): return f\"${x:,.2f}\"\n",
    "\n",
    "comparison_df = pd.DataFrame(global_data[\"results\"]).T\n",
    "comparison_df[\"Total Cost ($)\"] = comparison_df[\"Total Cost ($)\"].apply(fmt_money)\n",
    "comparison_df[\"Unmet Energy (kWh)\"] = comparison_df[\"Unmet Energy (kWh)\"].apply(lambda v: f\"{v:.3f}\")\n",
    "comparison_df[\"Curtailed Energy (kWh)\"] = comparison_df[\"Curtailed Energy (kWh)\"].apply(lambda v: f\"{v:.3f}\")\n",
    "comparison_df[\"Test Reward\"] = comparison_df[\"Test Reward\"].apply(lambda v: f\"{v:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ ALGORITHM COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microgrid-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
